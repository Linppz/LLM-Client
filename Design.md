# 设计决策

## 1. 工厂 + 单例模式（LLMFactory）

**方案**：用工厂模式创建 LLM 客户端，每个 provider 全局只有一个实例。

**为什么**：
- 减少了无效等待时间  
- 连接本身就可以复用
- api是无记忆模式的
- 限流器绑定，否则限流器就会失效

**如果不这样做**：
- 每次需要单独建立http时间会等待冗长的握手时间，在高并发的情况下会造成系统io拥挤

## 2. ABC 抽象基类（BaseLLM）

**方案**：
- 建立Base类，由于llm只有流式输出和一次性输出，所有继承类很好写
**为什么**：
- 易于拓展，用户可以自定义自己的api接口
- 易于继承，api仅仅只需要两种输出就好了，并且相互间的协议是通的，比如deepseek可以使用openai的协议
**如果不这样做**：
- 浪费了不同api之间遵从相等协议的便利性
- 每次单独部署新模型的代码成本上升


## 3. 弹性三件套（重试 + 限流 + 信号量）
**为什么**：
- 基于错误类型决定是否重试，有可能有些只是网络波动，所以需要重试
- 限流是限制某一时间区间最多多少请求
- 信号量是限制同一时刻最多并发数量
- 三点保证了连接的稳定性，且相互之间的功能没有重叠，所以不建议取消
**如果不这样做**：
- 影响客户使用体验
- 系统稳定性下降
- api访问稳定性下降

## 4. Jinja2 模板继承
**为什么**：
- 非常的快，因为其继承的特性
- 便于管理，易于统一处理
**如果不这样做**：
- 人为增加不必要的代码量

## 5. SHA256 Prompt 版本控制
**为什么**：
- 非常清晰的历史数据管理
- 可以帮助量化prompt的质量
- 如果收到llm的风控， 从prompt可以快速分析是否是内容问题
**如果不这样做**：
- 本质上代码和脚本没有区别，遇到问题无法分析


## 6. 结构化输出 + 重试 —— LLM 返回的不是 JSON 怎么办？
**为什么**：
- 强制添加符号转json
- 打回给llm重新处理，附加补充条件
**如果不这样做**：
- schema无法适配导致客户影响度受损


## 7. 全面使用 Pydantic —— 为什么不用普通 dict？
**为什么**：
- Pydantic 有自动检查，以及转类型
- 自定义数据类型
**如果不这样做**：
- 需要自己手工搓轮子，或者忍受数据有类型问题的风险